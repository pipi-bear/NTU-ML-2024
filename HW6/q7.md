# 7.

Plugging in $\mathbf{\alpha} = \mathbf{1}$ and $b = 0$ into $h_{\mathbf{\alpha}, b}(\mathbf{x})$, we get:

$$
\hat{h}(\mathbf{x}) = \mathrm{sign}\left(\sum_{n=1}^N y_n K(\mathbf{x}_n, \mathbf{x}) \right)
$$

Consider arbitrary $\mathbf{x}_n$ and $\mathbf{x}_m$, the result of the Gaussian kernel is:

$$
K(\mathbf{x}_n, \mathbf{x}_m) =
\begin{cases}
\exp(- \gamma \|\mathbf{x}_n - \mathbf{x}_m\|^2) &\qquad \text{if} \ n \ne m\\
1 &\qquad \text{if} \ n = m
\end{cases}
$$

Since the problem assumed that $||\mathbf{x}_n - \mathbf{x}_m|| \ge \epsilon \quad \forall n \ne m$, for the case $n \ne m$, we have:

$$
K(\mathbf{x}_n, \mathbf{x}_m) = \exp(- \gamma \|\mathbf{x}_n - \mathbf{x}_m\|^2) \le \exp(-\gamma \epsilon^2)
$$

Consider the condition that $\gamma > \frac{\ln(N-1)}{\epsilon^2}$, we have:

$$
\exp(- \gamma \|\mathbf{x}_n - \mathbf{x}_m\|^2) \le \exp(-\frac{\ln(N-1)}{\epsilon^2} \epsilon^2)  = \exp(-\ln(N-1)) = \frac{1}{N-1}
$$
Thus, $\exp(- \gamma \|\mathbf{x}_n - \mathbf{x}_m\|^2) \rightarrow 0$ when $\gamma$ sufficiently large.

Therefore, we can observe that when $\gamma$ is large enough, for the prediction of an arbitrary $\mathbf{x}_m$, we have:

$$
\begin{split}
\hat{h}(\mathbf{x}_m) 
&= \mathrm{sign}\left(\sum_{n=1}^N y_n K(\mathbf{x}_n, \mathbf{x}_m) \right) \\
&= \mathrm{sign}\left(\sum_{n \in \{1 \dots N\} \setminus m} y_n \exp(-\gamma \|\mathbf{x}_n - \mathbf{x}_m\|^2) + y_m \times 1 \right) \\
&\approx \mathrm{sign}\left(y_m \right)
\end{split}
$$

This means that the prediction made by $\hat{h}$ is the same as the label of $\mathbf{x}_m$, which results in $E_{in}(\hat{h}) = 0$.
